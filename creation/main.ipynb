{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80e4644-21ee-4682-8722-d448dec15414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   liquidity_lock  top5_holder_pct  top10_holder_pct  volume_spike_ratio  \\\n",
      "0               0         0.329202          0.450914            1.052517   \n",
      "1               1         0.254715          0.590134            0.888817   \n",
      "2               1         0.393910          0.360236            0.683118   \n",
      "3               0         0.808068          0.789527            1.053090   \n",
      "4               0         0.822637          0.602806            1.194389   \n",
      "\n",
      "   liquidity_drain_pct  comment_velocity  comment_spam_ratio  hype_score  \\\n",
      "0                 0.00                15            0.635036    0.861809   \n",
      "1                 0.00                 2            0.225824    0.473315   \n",
      "2                 0.00                 2            0.077117    0.556033   \n",
      "3                 0.95                 7            0.645193    0.627983   \n",
      "4                 0.95                11            0.829876    0.889861   \n",
      "\n",
      "   price_spike_magnitude  price_crash_depth  time_to_crash_min  \\\n",
      "0               3.772848           0.907934          17.509174   \n",
      "1               1.225156           0.395899         129.000874   \n",
      "2               1.005396           0.373496          27.119829   \n",
      "3               3.078293           0.984680          32.654874   \n",
      "4               6.288690           0.938983           5.804034   \n",
      "\n",
      "   bundled_rug_selloff  deployment_batch_size  dev_wallet_reputation  \\\n",
      "0                  1.0                    5.0                    0.0   \n",
      "1                  0.0                    1.0                    0.0   \n",
      "2                  0.0                    1.0                    0.0   \n",
      "3                  1.0                    3.0                    0.0   \n",
      "4                  0.0                    1.0                    1.0   \n",
      "\n",
      "   wallet_clustering_hhi  label  \n",
      "0               0.625096      0  \n",
      "1               0.540545      0  \n",
      "2               0.154773      0  \n",
      "3               0.830955      1  \n",
      "4               0.788736      1  \n",
      "\n",
      "Dataset shape: (1000000, 16)\n",
      "\n",
      "Label Distribution:\n",
      "label\n",
      "1    0.689956\n",
      "0    0.310044\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Parquet file\n",
    "df = pd.read_parquet('synthetic_scam_token_dataset.parquet')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Check the dataset shape\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "\n",
    "# Optional: Check label distribution (Scam vs Legit)\n",
    "print(\"\\nLabel Distribution:\")\n",
    "print(df['label'].value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae959155-366c-4754-b1be-4440a03ff7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class ScamTokenEnv(gym.Env):\n",
    "    def __init__(self, parquet_file):\n",
    "        super(ScamTokenEnv, self).__init__()\n",
    "\n",
    "        # Load dataset\n",
    "        self.data = pd.read_parquet(parquet_file).reset_index(drop=True)\n",
    "\n",
    "        # Extract features (exclude label)\n",
    "        self.features = self.data.drop(columns=['label']).values\n",
    "        self.labels = self.data['label'].values  # 1=scam, 0=legit\n",
    "\n",
    "        # Observation space: based on feature dimensions\n",
    "        self.observation_space = spaces.Box(\n",
    "        low=np.min(self.features, axis=0).astype(np.float32),\n",
    "        high=np.max(self.features, axis=0).astype(np.float32),\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "\n",
    "        # Action space: 0 = Wait, 1 = Flag, 2 = Classify Legit, 3 = Classify Scam\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "\n",
    "        # Environment state\n",
    "        self.current_step = 0\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs = self._get_observation()\n",
    "        true_label = self.labels[self.current_step]\n",
    "\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Reward logic\n",
    "        if action == 0:  # Wait\n",
    "            reward = -0.01  # Small penalty for delay\n",
    "        elif action == 1:  # Flag\n",
    "            reward = -0.005  # Slightly lower penalty\n",
    "        elif action == 2:  # Classify Legit\n",
    "            reward = 1 if true_label == 0 else -1\n",
    "            done = True\n",
    "        elif action == 3:  # Classify Scam\n",
    "            reward = 1 if true_label == 1 else -1\n",
    "            done = True\n",
    "\n",
    "        # Move to next sample\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.data):\n",
    "            done = True\n",
    "\n",
    "        next_obs = self._get_observation() if not done else np.zeros(self.observation_space.shape)\n",
    "        return next_obs, reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        return self.features[self.current_step].astype(np.float32)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}, Observation: {self._get_observation()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad9fd633-30c4-4dba-a754-0539773ab6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0, Observation: [ 0.          0.32920238  0.450914    1.0525174   0.         15.\n",
      "  0.6350358   0.86180943  3.772848    0.90793365 17.509174    1.\n",
      "  5.          0.          0.62509644]\n",
      "Action: 2, Reward: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment\n",
    "env = ScamTokenEnv('synthetic_scam_token_dataset.parquet')\n",
    "\n",
    "# Example interaction loop\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()  # Random action for testing\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    print(f\"Action: {action}, Reward: {reward}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef5724d-fd39-4cc5-ba99-d3cd7d5bd09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from scam_token_env import ScamTokenEnv  # Assuming your custom env is saved here\n",
    "!pip install shimmy\n",
    "\n",
    "# Initialize environment\n",
    "def make_env():\n",
    "    return ScamTokenEnv('synthetic_scam_token_dataset.parquet')\n",
    "\n",
    "env = DummyVecEnv([make_env])\n",
    "\n",
    "# Define DQN model\n",
    "model = DQN(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=100_000,\n",
    "    learning_starts=1_000,\n",
    "    batch_size=64,\n",
    "    tau=1.0,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1_000,\n",
    "    exploration_fraction=0.1,\n",
    "    exploration_final_eps=0.02,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=100_000)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean Reward: {mean_reward}, Std Reward: {std_reward}\")\n",
    "\n",
    "# Save model\n",
    "model.save(\"dqn_scam_token_detector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283868b3-2d97-4191-a292-bb4d7b9a6cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
